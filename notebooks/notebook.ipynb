{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e08432",
   "metadata": {},
   "source": [
    "\n",
    "# Market Basket Mining — Interactive CLI (Notebook Edition)\n",
    "\n",
    "This notebook wraps your CLI so you can run it directly here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454f77a0",
   "metadata": {},
   "source": [
    "## 1) Optional: install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f462930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, uncomment and run:\n",
    "# !pip install mlxtend pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10aa00e",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Expected project layout\n",
    "\n",
    "Place your CSVs in a `./data` folder next to this notebook:\n",
    "\n",
    "```\n",
    "project/\n",
    "├─ data/\n",
    "│  ├─ Amazon_Transactions.csv\n",
    "│  ├─ BestBuy_Transactions.csv\n",
    "│  ├─ KMart_Transactions.csv\n",
    "│  ├─ Generic_Transactions.csv\n",
    "│  └─ Nike_Transactions.csv\n",
    "└─ Market_Basket_Mining_CLI.ipynb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce3831",
   "metadata": {},
   "source": [
    "## 3) Program code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2414c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "# importing mlxtend; if missing, give a helpful message without crashing.\n",
    "try:\n",
    "    from mlxtend.preprocessing import TransactionEncoder\n",
    "    from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth\n",
    "except Exception:\n",
    "    print(\n",
    "        \"\\n[!] The 'mlxtend' package is required.\\n\"\n",
    "        \"    Install it first:\\n\"\n",
    "        \"        pip install mlxtend\\n\"\n",
    "    )\n",
    "    raise\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration & Data Loading\n",
    "# -----------------------------\n",
    "\n",
    "CANDIDATE_FILES = {\n",
    "    1: [\"Amazon_Transactions.csv\"],\n",
    "    2: [\"BestBuy_Transactions.csv\"],\n",
    "    3: [\"KMart_Transactions.csv\"],\n",
    "    4: [\"Generic_Transactions.csv\"],\n",
    "    5: [\"Nike_Transactions.csv\"],\n",
    "}\n",
    "\n",
    "MENU = (\n",
    "    \"Select your Dataset:\\n\"\n",
    "    \"  1. Amazon\\n\"\n",
    "    \"  2. BestBuy\\n\"\n",
    "    \"  3. K-Mart\\n\"\n",
    "    \"  4. Generic\\n\"\n",
    "    \"  5. Nike\\n\"\n",
    "    \"  0. Exit\\n\"\n",
    "    \"Enter choice (0-5): \"\n",
    ")\n",
    "\n",
    "try:\n",
    "    __file__\n",
    "    SCRIPT_DIR = Path(__file__).parent\n",
    "except NameError:\n",
    "    SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Prefer ./data (same folder as notebook) when present; otherwise ../data\n",
    "if (SCRIPT_DIR / \"data\").exists():\n",
    "    PROJECT_ROOT = SCRIPT_DIR\n",
    "else:\n",
    "    PROJECT_ROOT = SCRIPT_DIR.parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "def print_header(title: str):\n",
    "    bar = \"=\" * len(title)\n",
    "    print(f\"\\n{title}\\n{bar}\")\n",
    "\n",
    "def find_existing_path(candidates):\n",
    "    \"\"\"Return the first existing file path from the candidates list.\"\"\"\n",
    "    for name in candidates:\n",
    "        p = DATA_DIR / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def read_csv_safely(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read CSV with sensible defaults and fallback encoding.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin-1\")\n",
    "\n",
    "# -----------------------------\n",
    "# Transaction Extraction\n",
    "# -----------------------------\n",
    "\n",
    "LIKELY_BASKET_COLS = [\"Transaction\", \"Items\", \"ItemList\", \"Basket\", \"Products\"]\n",
    "ID_COL_CANDIDATES = [\"TransactionID\", \"TransID\", \"InvoiceNo\", \"OrderID\", \"TID\", \"BasketID\", \"TxnID\"]\n",
    "ITEM_COL_CANDIDATES = [\"Item\", \"Items\", \"Product\", \"Description\", \"SKU\"]\n",
    "DELIMS = [\",\", \";\", \"|\"]\n",
    "\n",
    "def is_delimited_basket_series(series: pd.Series) -> bool:\n",
    "    \"\"\"Heuristic: a string column where many rows contain a delimiter.\"\"\"\n",
    "    if series.dtype == object:\n",
    "        sample = series.dropna().astype(str).head(200)\n",
    "        if sample.empty:\n",
    "            return False\n",
    "        hits = sum(1 for val in sample if any(d in val for d in DELIMS))\n",
    "        return hits >= max(5, len(sample) // 5)\n",
    "    return False\n",
    "\n",
    "def extract_transactions(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Return list[list[str]] transactions from:\n",
    "    1) One row per transaction, with a delimited items column\n",
    "    2) One row per (transaction_id, item), grouped\n",
    "    \"\"\"\n",
    "    # 1) Direct basket column\n",
    "    for col in LIKELY_BASKET_COLS:\n",
    "        if col in df.columns and is_delimited_basket_series(df[col]):\n",
    "            baskets = []\n",
    "            for raw in df[col].fillna(\"\"):\n",
    "                s = str(raw)\n",
    "                delim = max(DELIMS, key=lambda d: s.count(d))\n",
    "                parts = [p.strip() for p in s.split(delim) if p.strip()]\n",
    "                if parts:\n",
    "                    baskets.append(parts)\n",
    "            if baskets:\n",
    "                return baskets\n",
    "\n",
    "    # 2) (transaction_id, item)\n",
    "    id_col = next((c for c in ID_COL_CANDIDATES if c in df.columns), None)\n",
    "    item_col = next((c for c in ITEM_COL_CANDIDATES if c in df.columns), None)\n",
    "    if id_col and item_col:\n",
    "        grouped = (\n",
    "            df[[id_col, item_col]]\n",
    "            .dropna()\n",
    "            .astype({id_col: str, item_col: str})\n",
    "            .groupby(id_col)[item_col]\n",
    "            .apply(lambda s: [x.strip() for x in s if str(x).strip()])\n",
    "        )\n",
    "        baskets = [lst for lst in grouped.tolist() if lst]\n",
    "        if baskets:\n",
    "            return baskets\n",
    "\n",
    "    # 3) Last resort: any object column that looks delimited\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object and is_delimited_basket_series(df[col]):\n",
    "            baskets = []\n",
    "            for raw in df[col].fillna(\"\"):\n",
    "                s = str(raw)\n",
    "                delim = max(DELIMS, key=lambda d: s.count(d))\n",
    "                parts = [p.strip() for p in s.split(delim) if p.strip()]\n",
    "                if parts:\n",
    "                    baskets.append(parts)\n",
    "            if baskets:\n",
    "                return baskets\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Could not detect transactions. Use a delimited items column \"\n",
    "        \"(e.g., 'Items' = 'A,B,C') OR a pair like (TransactionID, Item).\"\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Brute-force Apriori (baseline)\n",
    "# -----------------------------\n",
    "\n",
    "def brute_force_apriori(transactions, min_support: float):\n",
    "    \"\"\"\n",
    "    Simple Apriori-like baseline using combinations over unique items.\n",
    "    Returns frequent_patterns (list[tuple]) and pattern_counts (list[int]).\n",
    "    \"\"\"\n",
    "    unique_items = sorted({itm for tx in transactions for itm in tx})\n",
    "    n_tx = len(transactions)\n",
    "\n",
    "    def support_count(itemset):\n",
    "        s = set(itemset)\n",
    "        return sum(1 for tx in transactions if s.issubset(tx))\n",
    "\n",
    "    k = 1\n",
    "    current_items = unique_items[:]\n",
    "    frequent_patterns, pattern_counts = [], []\n",
    "\n",
    "    while current_items:\n",
    "        candidate_itemsets = list(combinations(current_items, k))\n",
    "        new_freq = []\n",
    "        for cand in candidate_itemsets:\n",
    "            cnt = support_count(cand)\n",
    "            if cnt >= min_support * n_tx:\n",
    "                new_freq.append(cand)\n",
    "                frequent_patterns.append(cand)\n",
    "                pattern_counts.append(cnt)\n",
    "        items_in_new = sorted({i for it in new_freq for i in it})\n",
    "        k += 1\n",
    "        current_items = items_in_new\n",
    "\n",
    "    return frequent_patterns, pattern_counts\n",
    "\n",
    "def generate_rules_from_patterns(frequent_patterns, pattern_counts, n_tx, min_conf: float):\n",
    "    \"\"\"\n",
    "    Rule generation from frequent patterns.\n",
    "    Returns list of tuples: (antecedent, consequent, support, confidence)\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    supp_map = {tuple(sorted(p)): c / n_tx for p, c in zip(frequent_patterns, pattern_counts)}\n",
    "\n",
    "    for pattern, supp_cnt in zip(frequent_patterns, pattern_counts):\n",
    "        if len(pattern) < 2:\n",
    "            continue\n",
    "        pat_set = set(pattern)\n",
    "        for r in range(1, len(pattern)):\n",
    "            for antecedent in combinations(pattern, r):\n",
    "                consequent = tuple(sorted(pat_set - set(antecedent)))\n",
    "                antecedent = tuple(sorted(antecedent))\n",
    "                supp_XY = supp_map.get(tuple(sorted(pattern)), supp_cnt / n_tx)\n",
    "                supp_X = supp_map.get(antecedent, None)\n",
    "                if not supp_X:\n",
    "                    continue\n",
    "                conf = supp_XY / supp_X\n",
    "                if conf + 1e-12 >= min_conf:\n",
    "                    rules.append((antecedent, consequent, supp_XY, conf))\n",
    "    # Sort by confidence desc, then support desc\n",
    "    rules.sort(key=lambda x: (x[3], x[2]), reverse=True)\n",
    "    return rules\n",
    "\n",
    "# -----------------------------\n",
    "# Pretty Printing (print ALL)\n",
    "# -----------------------------\n",
    "\n",
    "def print_frequent_patterns(frequent_patterns, pattern_counts, n_tx):\n",
    "    print_header(\"Frequent Patterns\")\n",
    "    rows = []\n",
    "    for pat, cnt in zip(frequent_patterns, pattern_counts):\n",
    "        supp = cnt / n_tx\n",
    "        rows.append((pat, supp))\n",
    "    rows.sort(key=lambda x: (x[1], len(x[0])), reverse=True)\n",
    "    if not rows:\n",
    "        print(\"(none)\")\n",
    "        return\n",
    "    for i, (pat, supp) in enumerate(rows, start=1):\n",
    "        print(f\"[{i:>3}] {list(pat)} | support={supp:.4f}\")\n",
    "\n",
    "def print_rules(rules):\n",
    "    print_header(\"Association Rules\")\n",
    "    if not rules:\n",
    "        print(\"(no rules met the thresholds)\")\n",
    "        return\n",
    "    for i, (ante, cons, supp, conf) in enumerate(rules, start=1):\n",
    "        print(\n",
    "            f\"[{i:>3}] {list(ante)}  ->  {list(cons)}  \"\n",
    "            f\"| support={supp:.4f}, confidence={conf:.4f}\"\n",
    "        )\n",
    "\n",
    "# -----------------------------\n",
    "# mlxtend Pipelines\n",
    "# -----------------------------\n",
    "\n",
    "def encode_transactions(transactions):\n",
    "    te = TransactionEncoder()\n",
    "    arr = te.fit(transactions).transform(transactions)\n",
    "    df_enc = pd.DataFrame(arr, columns=te.columns_)\n",
    "    return df_enc\n",
    "\n",
    "def run_mlxtend_apriori(df_enc, min_support, min_confidence):\n",
    "    start = time.time()\n",
    "    fi = apriori(df_enc, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(fi, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    runtime = time.time() - start\n",
    "    return fi, rules, runtime\n",
    "\n",
    "def run_mlxtend_fpgrowth(df_enc, min_support, min_confidence):\n",
    "    start = time.time()\n",
    "    fi = fpgrowth(df_enc, min_support=min_support, use_colnames=True)\n",
    "    rules = association_rules(fi, metric=\"confidence\", min_threshold=min_confidence)\n",
    "    runtime = time.time() - start\n",
    "    return fi, rules, runtime\n",
    "\n",
    "def print_mlxtend_outputs(name, fi, rules):\n",
    "    # ---- Frequent Itemsets ----\n",
    "    print_header(f\"{name} — Frequent Itemsets\")\n",
    "    if fi.empty:\n",
    "        print(\"(none)\")\n",
    "    else:\n",
    "        fi2 = fi.copy()\n",
    "        fi2[\"k\"] = fi2[\"itemsets\"].map(len)  # ensure sort key is a column\n",
    "        fi_sorted = fi2.sort_values([\"support\", \"k\"], ascending=[False, False])\n",
    "        for i, row in enumerate(fi_sorted.itertuples(index=False), start=1):\n",
    "            print(f\"[{i:>3}] {sorted(list(row.itemsets))} | support={row.support:.4f}\")\n",
    "\n",
    "    # ---- Rules (support & confidence only) ----\n",
    "    print_header(f\"{name} — Association Rules\")\n",
    "    if rules.empty:\n",
    "        print(\"(none)\")\n",
    "    else:\n",
    "        # Sort by confidence then support if present\n",
    "        sort_cols = [c for c in [\"confidence\", \"support\"] if c in rules.columns]\n",
    "        rules_sorted = rules.sort_values(sort_cols, ascending=[False, False]) if sort_cols else rules\n",
    "        for i, row in enumerate(rules_sorted.itertuples(index=False), start=1):\n",
    "            ants = sorted(list(getattr(row, \"antecedents\", [])))\n",
    "            cons = sorted(list(getattr(row, \"consequents\", [])))\n",
    "            support = getattr(row, \"support\", float(\"nan\"))\n",
    "            conf = getattr(row, \"confidence\", float(\"nan\"))\n",
    "            print(\n",
    "                f\"[{i:>3}] {ants} -> {cons}  \"\n",
    "                f\"| support={support:.4f}, confidence={conf:.4f}\"\n",
    "            )\n",
    "\n",
    "# -----------------------------\n",
    "# Defensive Input Helpers\n",
    "# -----------------------------\n",
    "\n",
    "def ask_menu_choice() -> int:\n",
    "    \"\"\"Ask for a dataset choice (0–5). Loops until valid.\"\"\"\n",
    "    while True:\n",
    "        raw = input(MENU).strip()\n",
    "        if raw == \"\":\n",
    "            print(\"Please enter a number between 0 and 5.\")\n",
    "            continue\n",
    "        if not raw.isdigit():\n",
    "            print(\"Invalid input. Enter a number between 0 and 5.\")\n",
    "            continue\n",
    "        val = int(raw)\n",
    "        if 0 <= val <= 5:\n",
    "            return val\n",
    "        print(\"Out of range. Enter a number between 0 and 5.\")\n",
    "\n",
    "def ask_float(prompt: str, lo=0.0, hi=1.0) -> float:\n",
    "    \"\"\"Ask for a float in [lo, hi]. Loops until valid.\"\"\"\n",
    "    while True:\n",
    "        raw = input(prompt).strip()\n",
    "        if raw == \"\":\n",
    "            print(f\"Please enter a value between {lo} and {hi}.\")\n",
    "            continue\n",
    "        try:\n",
    "            if raw.startswith(\".\"):\n",
    "                raw = \"0\" + raw\n",
    "            val = float(raw)\n",
    "        except ValueError:\n",
    "            print(\"Invalid number. Try again.\")\n",
    "            continue\n",
    "        if lo <= val <= hi:\n",
    "            return val\n",
    "        print(f\"Out of range. Enter a value between {lo} and {hi}.\")\n",
    "\n",
    "def ask_yes_no(prompt: str) -> bool:\n",
    "    \"\"\"Ask a yes/no question (y/n).\"\"\"\n",
    "    while True:\n",
    "        raw = input(prompt).strip().lower()\n",
    "        if raw in (\"y\", \"yes\"):\n",
    "            return True\n",
    "        if raw in (\"n\", \"no\"):\n",
    "            return False\n",
    "        print(\"Please answer with 'y' or 'n'.\")\n",
    "\n",
    "# -----------------------------\n",
    "# One Run (for a chosen dataset)\n",
    "# -----------------------------\n",
    "\n",
    "def do_one_run(choice: int):\n",
    "    \"\"\"Execute the mining pipeline for the selected dataset number.\"\"\"\n",
    "    path = find_existing_path(CANDIDATE_FILES[choice])\n",
    "    if not path:\n",
    "        print(\"\\n[!] Could not find any of the expected files for your choice:\")\n",
    "        for candidate in CANDIDATE_FILES[choice]:\n",
    "            print(f\"    - {candidate}\")\n",
    "        print(\"    Place the CSV next to this .py file and try again.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n[+] Loading: {path.name}\")\n",
    "    df = read_csv_safely(path)\n",
    "\n",
    "    try:\n",
    "        transactions = extract_transactions(df)\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[!] Failed to parse transactions: {e}\")\n",
    "        return\n",
    "\n",
    "    n_tx = len(transactions)\n",
    "    unique_items = sorted({itm for tx in transactions for itm in tx})\n",
    "    print(f\"[i] Parsed {n_tx} transactions with {len(unique_items)} unique items.\")\n",
    "\n",
    "    min_support = ask_float(\"\\nInput your minimum support value (0-1), e.g. 0.02: \", 0.0, 1.0)\n",
    "    min_confidence = ask_float(\"Input your minimum confidence value (0-1), e.g. 0.4: \", 0.0, 1.0)\n",
    "\n",
    "    # ---- Brute-force baseline ----\n",
    "    t0 = time.time()\n",
    "    frequent_patterns, pattern_counts = brute_force_apriori(transactions, min_support)\n",
    "    rules_bf = generate_rules_from_patterns(frequent_patterns, pattern_counts, n_tx, min_confidence)\n",
    "    rt_bf = time.time() - t0\n",
    "\n",
    "    print_frequent_patterns(frequent_patterns, pattern_counts, n_tx)\n",
    "    print_rules(rules_bf)\n",
    "    print(f\"\\n[Runtime] Brute-force Apriori: {rt_bf:.4f} s\")\n",
    "\n",
    "    # ---- mlxtend pipelines ----\n",
    "    df_enc = encode_transactions(transactions)\n",
    "\n",
    "    fi_ap, rules_ap, rt_ap = run_mlxtend_apriori(df_enc, min_support, min_confidence)\n",
    "    print_mlxtend_outputs(\"Apriori (mlxtend)\", fi_ap, rules_ap)\n",
    "    print(f\"\\n[Runtime] mlxtend Apriori: {rt_ap:.4f} s\")\n",
    "\n",
    "    fi_fp, rules_fp, rt_fp = run_mlxtend_fpgrowth(df_enc, min_support, min_confidence)\n",
    "    print_mlxtend_outputs(\"FP-Growth (mlxtend)\", fi_fp, rules_fp)\n",
    "    print(f\"\\n[Runtime] mlxtend FP-Growth: {rt_fp:.4f} s\")\n",
    "\n",
    "    # ---- Timing Summary (no speed column) ----\n",
    "    bf_itemsets = len(frequent_patterns)\n",
    "    bf_rules = len(rules_bf)\n",
    "    ap_itemsets = 0 if fi_ap is None or fi_ap.empty else int(fi_ap.shape[0])\n",
    "    ap_rules = 0 if rules_ap is None or rules_ap.empty else int(rules_ap.shape[0])\n",
    "    fp_itemsets = 0 if fi_fp is None or fi_fp.empty else int(fi_fp.shape[0])\n",
    "    fp_rules = 0 if rules_fp is None or rules_fp.empty else int(rules_fp.shape[0])\n",
    "\n",
    "    print_header(\"Timing Summary\")\n",
    "    print(f\"Dataset: {path.name} | Transactions: {n_tx} | Unique items: {len(unique_items)}\\n\")\n",
    "    print(f\"{'Algorithm':<20} {'Itemsets':>9} {'Rules':>9} {'Runtime (s)':>14}\")\n",
    "    print(\"-\" * 56)\n",
    "    print(f\"{'Brute-force Apriori':<20} {bf_itemsets:>9} {bf_rules:>9} {rt_bf:>14.4f}\")\n",
    "    print(f\"{'Apriori (mlxtend)':<20} {ap_itemsets:>9} {ap_rules:>9} {rt_ap:>14.4f}\")\n",
    "    print(f\"{'FP-Growth (mlxtend)':<20} {fp_itemsets:>9} {fp_rules:>9} {rt_fp:>14.4f}\")\n",
    "\n",
    "    print_header(\"Done\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main Loop\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    print_header(\"Data Mining Midterm Project\")\n",
    "    try:\n",
    "        while True:\n",
    "            choice = ask_menu_choice()\n",
    "            if choice == 0:   # Exit\n",
    "                print(\"Goodbye!\")\n",
    "                return\n",
    "            do_one_run(choice)\n",
    "            print()  # spacing\n",
    "            if not ask_yes_no(\"Run again? (y/n): \"):\n",
    "                print(\"Goodbye!\")\n",
    "                return\n",
    "            print()  # spacing\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nInterrupted. Goodbye!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b63d784",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Run the app\n",
    "\n",
    "Execute the cell below and follow the prompts in the output.  \n",
    "Use `0` at the dataset menu to exit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1dc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch interactive loop\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except SystemExit:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}